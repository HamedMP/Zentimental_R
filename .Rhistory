setwd("~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1")
runApp(../shiny1)
library(shiny)
library(shiny)
runApp(../shiny1)
runApp('../shiny1')
runApp('../shiny1')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
setwd("~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4")
runApp(shiny1)
runApp('shiny1')
library(shiny)
runApp('shiny1')
runApp('shiny1')
runApp()
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
runApp('shiny1')
library(tm)
install.packages("wordclout")
install.packages("wordcloud")
library(wordcloud)
processWords <- function(word = "datascience",
nbTweets = 200){
tweets <-searchTwitter(word, n=nbTweets)
tweetTxt <- sapply(tweets, function(x) x$getText())
tweetTxt <- sapply(tweetTxt,
function(row) iconv(
row,
from = 'latin 1',
to = 'ASCII',
sub = ""
))
myCorpus <- Corpus(VectorSource(tweetTxt))
tdm <- TermDocumentMatrix(myCorpus, control = list(removePunctuation = True,
stopwords = c(stopwords("english")),
removeNumbers = TRUE,
tolower = TRUE))
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
dm <- data.frame(word=names(word_freqs), freq = word_freqs)
wordcloud(dm)
}
source("helpers.R")
setwd("~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1")
source("helpers.R")
dm, wf = processWords()
words = processWords()
connectTwitter()
words = processWords()
dm = words[1]
words = processWords()
source("helpers.R")
words = processWords()
source("helpers.R")
words = processWords()
dm = words[1]
word_freqa = words[2]
word_freqs = words[2]
wordcloud(words = dm, freq = word_freqs)
wordcloud(words = dm)
source("helpers.R")
words = processWords()
wordcloud(words = words)
wordcloud(words, max.words = 100)
head(d, 10)
processWords <- function(word = "datascience",
nbTweets = 200){
tweets <-searchTwitter(word, n=nbTweets)
tweetTxt <- sapply(tweets, function(x) x$getText())
tweetTxt <- sapply(tweetTxt,
function(row) iconv(
row,
from = 'latin1',
to = 'ASCII',
sub = ""
))
myCorpus <- Corpus(VectorSource(tweetTxt))
tdm <- TermDocumentMatrix(myCorpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("english")),
removeNumbers = TRUE,
tolower = TRUE))
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
dm <- data.frame(word=names(word_freqs), freq = word_freqs)
head(d, 10)
return(dm)
}
head(d, 10)
tweets <-searchTwitter(word, n=nbTweets)
word = "datascience"
nbTweets = 200
nbTweets = 200
tweets <-searchTwitter(word, n=nbTweets)
tweetTxt <- sapply(tweets, function(x) x$getText())
tweetTxt <- sapply(tweetTxt,
function(row) iconv(
row,
from = 'latin1',
to = 'ASCII',
sub = ""
))
myCorpus <- Corpus(VectorSource(tweetTxt))
tdm <- TermDocumentMatrix(myCorpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("english")),
removeNumbers = TRUE,
tolower = TRUE))
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
dm <- data.frame(word=names(word_freqs), freq = word_freqs)
head(d, 10)
head(dm, 10)
source("helpers.R")
words = processWords()
wordcloud(words)
head(dm, 10)
wordcloud(dm)
wordcloud(dm$word, freq = dm$freq)
library(colorspace)
wordcloud(words$word, freq = words$freq, colors = brewer.pal(8, "Dark2"))
wordcloud(words$word, freq = words$freq, colors = brewer.pal(8, "Dark2"),
max.words = 20)
wordcloud(words$word, freq = words$freq, colors = brewer.pal(8, "Dark2"),
max.words = 50)
runApp()
shiny::runApp()
runApp()
install.packages("entropy")
library(entropy)
merge(p,q, by.x = "word", by.y = "word")
}
KLD <- function(p, q) {
return(KL.plugin(p, q))
}
merge_keywords <- function(p, q){
merge(p,q, by.x = "word", by.y = "word")
}
KLD <- function(p, q) {
return(KL.plugin(p, q))
}
connectTwitter()
p = processWords()
q = processWords(word = "datamining")
p.top50 = chooseTopN("datascience", p, n=50)
q.top50 = chooseTopN("datamining", q, n=50)
merged = merge_keywords(p.top50, q.top50)
View(merged)
View(merged)
KLD(merged)
KLD(merged$freq.x, merged$freq.y)
q.top30 = chooseTopN("datamining", q, n=30)
p.top30 = chooseTopN("datascience", p, n=30)
merged30 = merge_keywords(p.top30, q.top30)
KLD(merged30$freq.x, merged30$freq.y)
KLD(merged30$freq.y, merged30$freq.x)
entropy(merged$freq.x)
entropy(merged$freq.y)
entropy(merged$word)
entropy(p$freq)
KLD(p$freq, q$freq)
set.seed(3951824)
X <- sample(letters[1:3], 100, 1)
Y <- sample(letters[4:5], 100, 1)
t  <- table(X,Y)
t
X
Y
X[X == "a"]
X == "a"
X[X == "a",]
match(X, "a")
tp  <- t/100 # proportions
tp
tn  <- tp/sum(tp)     # normalized, joints
tn
p_x <- rowSums(tn)    # marginals
p_y <- colSums(tn)
p_x
P <- tn
Q <- p_x %o% p_y
Q
p_x
p_y
mi <- sum(P*log(P/Q))
mi
library(entropy)
mi.empirical(t) == mi
P
Q
X
p_x <- rowSums(tn)    # marginals
p_y <- colSums(tn)
t <- table(p.top30, q.top30)
t <- table(p.top30$freq, q.top30$freq)
tp <- t / 30
tn <- tp/sum(tp)
p_x = rowSums(tn)
p_x
tn
p_x = colSums(tn)
P <- tn
Q =  p_x %o% p_y
mi <- sum(P*log(P/Q))
Q
p_x
p_y
p_y <- colSums(tn)
P <- tn
Q <- p_x %o% p_y
mi <- sum(P*log(P/Q))
p_y
p_x
p_x = rowSums(tn)
mi <- sum(P*log(P/Q))
p_x
mi <- sum(P*log(P/Q))
P*log(P/Q)
t = table(p.top30$freq, q.top30$freq)
tp  <- t/100 # proportions
tn  <- tp/sum(tp)     # normalized, joints
p_x <- rowSums(tn)    # marginals
p_y <- colSums(tn)
P <- tn
Q <- p_x %o% p_y
mi <- sum(P*log(P/Q))
library(entropy)
mi.empirical(t) == mi
mi
p_x
p_y
P
mi
View(merged30)
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/mi.R')
computeKLD(merged30$freq.x, merged30$freq.y)
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/mi.R')
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/mi.R')
computeKLD(merged30$freq.x, merged30$freq.y)
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/mi.R')
computeKLD(merged30$freq.x, merged30$freq.y)
tweets <-searchTwitter(word, n=nbTweets)
tweets <-searchTwitter("datascience", n=200)
tweetTxt <- sapply(tweets, function(x) x$getText())
tweetTxt
tweetsPen <-searchTwitter("pen", n=200)
tweetTxtPen <- sapply(tweets, function(x) x$getText())
tweetTxtPen
tweetTxtPen <- sapply(tweetsPen, function(x) x$getText())
tweetTxtPen
tweetTxt <- sapply(tweetTxt,
function(row) iconv(
row,
from = 'latin1',
to = 'ASCII',
sub = ""
))
myCorpus <- Corpus(VectorSource(tweetTxt))
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus
myCorpus$`23`
myCorpus$23
tdm <- TermDocumentMatrix(myCorpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("english"), stopwords("french")),
removeNumbers = TRUE,
tolower = TRUE))
tdm
tdm$i
tdm$j
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/mi.R')
computeKLD()
a = c(1,2,1)
b = c(1,2,3)
table(a,b)
m <- as.matrix(tdm)
m
X <- sample(letters[1:3], 100, 1)
Y <- sample(letters[4:5], 100, 1)
t  <- table(X,Y)
tp  <- t/100 # proportions
tp
t
tn  <- tp/sum(tp)     # normalized, joints
tn
X <- sample(letters[1:3], 100, 1)
Y <- sample(letters[4:5], 100, 1)
t  <- table(X,Y)
tp  <- t/100 # proportions
tn  <- tp/sum(tp)     # normalized, joints
p_x <- rowSums(tn)    # marginals
p_y <- colSums(tn)
P <- tn
Q <- p_x %o% p_y
mi <- sum(P*log(P/Q))
library(entropy)
mi.empirical(t) == mi
? mi.empirical
processWords <- function(word = "datascience",
nbTweets = 200){
tweets <-searchTwitter(word, n=nbTweets)
tweetTxt <- sapply(tweets, function(x) x$getText())
tweetTxt <- sapply(tweetTxt,
function(row) iconv(
row,
from = 'latin1',
to = 'ASCII',
sub = ""
))
myCorpus <- Corpus(VectorSource(tweetTxt))
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
tdm <- TermDocumentMatrix(myCorpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("english"), stopwords("french")),
removeNumbers = TRUE,
tolower = TRUE))
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
dm <- data.frame(word=names(word_freqs), freq = word_freqs)
head(dm, 10)
return(dm)
}
dm = processWords()
dm
dm2 = processWords(word = "datamining")
summary(dm2)
summary(dm)
P1 = chooseTopN("datascience", dm, n = 30)
P2 = chooseTopN("datamining", dm2, n = 30)
Q1 = merge(P1, dm2, by.x = "word", by.y = "word")
Q2 = merge(P2, dm1, by.x = "word", by.y = "word")
Q2 = merge(P2, dm, by.x = "word", by.y = "word")
Q1
Q2
P1
table(P1$freq, Q1$freq.y)
mi <- sum(P1*log(P1/Q1))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
dm
runApp()
p1 = dm$freq / 30
q1 = dm2$freq / 30
p1
p1 = p1/sum(p1)
q1 = q1/sum(q1)
library(entropy)
KL.plugin(p1, q1)
dm = chooseTopN("datascience", dm, n = 30)
dm2 = chooseTopN("datamining", dm2, n = 30)
p1 = dm$freq / 30
q1 = dm2$freq / 30
p1 = p1/sum(p1)
q1 = q1/sum(q1)
KL.plugin(p1, q1)
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/helpers.R')
calc_dlk("trump", "volvo")
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/helpers.R')
calc_dlk("trump", "volvo")
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/helpers.R')
calc_dlk("trump", "volvo")
calc_dlk("airplane", "سلام")
calc_dlk("airplane", "helloم")
calc_dlk("airplane", "hello")
calc_dlk("airplane", "aviation")
kl = calc_dlk("airplane", "aviation")
kl
kl.2
kl = calc_dlk("marine", "le pen")
kl
kl = calc_dlk("spain", "europe")
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/helpers.R')
source('~/Documents/UNS_PROJ/UNS_Data_Valorization/lab4/shiny1/helpers.R')
kl
kl = calc_dlk("spain", "europe")
shiny::runApp()
runApp()
runApp()
runApp()
